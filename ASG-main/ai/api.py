# app.py
# ğŸ§  AI Security Guard (ASG) â€” Video Analysis API

import os, sys, csv, cv2, numpy as np, torch, warnings, uuid, threading, shutil, subprocess
from collections import deque
from flask import Flask, request, jsonify, send_from_directory, abort, url_for
from pytorchvideo.models.hub import slowfast_r50
from ultralytics import YOLO
from os.path import basename
from flask_cors import CORS  # ğŸ”¥ ì¶”ê°€

# ğŸ”— DB / Models (User ì•ˆ ì”€)
import sys, os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from server.models import db, Job, Clip


# ================== Flask & DB ì´ˆê¸°í™” ==================
app = Flask(__name__)

DB_PATH = os.path.join(
    os.path.dirname(__file__), "..", "server", "instance", "users.db"
)
DB_PATH = os.path.abspath(DB_PATH)

app.config["SQLALCHEMY_DATABASE_URI"] = f"sqlite:///{DB_PATH}"


CORS(
    app,
    resources={r"/*": {"origins": ["http://localhost:3000", "http://127.0.0.1:3000"]}},
    supports_credentials=True,
)

db.init_app(app)

with app.app_context():
    db.create_all()

# ================== (ì›ë³¸) Config ê·¸ëŒ€ë¡œ ==================
CKPT = r".\result\ckpt.pt"
YOLO_WEIGHTS = r".\result\best.pt"
SAVE_VIDEO = True
SAVE_CSV = False

# --- (ì„ì‹œ) ë¹„í™œì„±í™”í•  í´ë˜ìŠ¤ë“¤ ---
DISABLE_CLASSES = {"vandalism"}

# --- Detection / ROI gating ---
PERSON_CONF = 0.55
MIN_AREA_RATIO = 0.0035
ASPECT_MIN, ASPECT_MAX = 0.25, 0.90
NO_PERSON_CLEAR = 8
ROI_SCALE = 1.45
ROI_MIN_AREA_RATIO = 0.02
BG_MODE = "gray"  # or "blur"

# --- SlowFast sampling & norm ---
warnings.filterwarnings(
    "ignore", category=FutureWarning, message="You are using torch.load"
)
MEAN = torch.tensor([0.45, 0.45, 0.45]).view(1, 3, 1, 1, 1)
STD = torch.tensor([0.225, 0.225, 0.225]).view(1, 3, 1, 1, 1)

# --- Temporal Stabilization ---
LOGIT_EMA = 0.70
MARGIN_MIN = 0.08
SWITCH_DELTA = 0.10
SWITCH_CONSEC = 4
MIN_HOLD = {
    "assault": 32,
    "swoon": 60,
    "trespass": 40,
}
MIN_SHOW_CONF = 0.30

# --- Fallbacks ---
T_FAST_FALLBACK = 32
ALPHA_FALLBACK = 4
SIZE_FALLBACK = 224

# ================== APIìš© ê²½ë¡œ (OUT_DIRë¥¼ ë£¨íŠ¸ë¡œ ì‚¬ìš©) ==================
BASE_OUT = os.path.abspath("./")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# íŒŒì¼ ë§¨ ìœ„ìª½ ì„¤ì •ë¶€
# EVENT_CLIPS_DIR = r"D:\PycharmProjects\pythonProject1\ASG-main\ai\event_clips"
# THUMBS_DIR      = r"D:\PycharmProjects\pythonProject1\ASG-main\ai\thumbnails"

EVENT_CLIPS_DIR = os.path.join(BASE_OUT, "event_clips")
THUMBS_DIR = os.path.join(BASE_OUT, "thumbnails")
UPLOAD_DIR = os.path.join(BASE_OUT, "uploads")
ANALYZED_DIR = os.path.join(BASE_OUT, "analyzed_videos")
CSV_DIR = os.path.join(BASE_OUT, "csv_logs")

for d in [BASE_OUT, UPLOAD_DIR, ANALYZED_DIR, EVENT_CLIPS_DIR, THUMBS_DIR, CSV_DIR]:
    os.makedirs(d, exist_ok=True)

URL_BASE = "http://127.0.0.1:5001"


# ================== ìœ í‹¸ í•¨ìˆ˜ ==================
def center_crop_rgb(bgr, size):
    img = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
    h, w = img.shape[:2]
    scale = 256.0 / max(1, min(h, w))
    nh, nw = int(round(h * scale)), int(round(w * scale))
    img = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)
    y0, x0 = max(0, (nh - size) // 2), max(0, (nw - size) // 2)
    img = img[y0 : y0 + size, x0 : x0 + size]
    return img


def expand_box(box, scale, W, H):
    x1, y1, x2, y2 = box
    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
    bw, bh = (x2 - x1) * scale, (y2 - y1) * scale
    nx1, ny1 = max(0, int(cx - bw / 2)), max(0, int(cy - bh / 2))
    nx2, ny2 = min(W, int(cx + bw / 2)), min(H, int(cy + bh / 2))
    return nx1, ny1, nx2, ny2


def make_roi_frame(frame, dets, scale=1.45, roi_min_ratio=0.02, bg_mode="gray"):
    H, W = frame.shape[:2]
    if not dets:
        return None
    mask = np.zeros((H, W), np.uint8)
    for (x1, y1, x2, y2, *_) in dets:
        ex1, ey1, ex2, ey2 = expand_box((x1, y1, x2, y2), scale, W, H)
        mask[ey1:ey2, ex1:ex2] = 255
    if mask.mean() / 255.0 < roi_min_ratio:
        return None
    if bg_mode == "blur":
        bg = cv2.GaussianBlur(frame, (0, 0), sigmaX=9, sigmaY=9)
    else:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        bg = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
    m3 = cv2.merge([mask, mask, mask])
    return np.where(m3 == 255, frame, bg)


def yolo_person_boxes(yolo, frame, conf, min_area_ratio, aspect_min, aspect_max):
    r = yolo.predict(source=frame, verbose=False, conf=conf, imgsz=640)[0]
    names = getattr(r, "names", {0: "person"})
    person_ids = {i for i, n in names.items() if str(n).lower() == "person"}
    H, W = frame.shape[:2]
    min_area = min_area_ratio * (H * W)
    out = []
    for b, c, s in zip(
        r.boxes.xyxy.cpu().numpy(),
        r.boxes.cls.cpu().numpy(),
        r.boxes.conf.cpu().numpy(),
    ):
        if (person_ids and int(c) not in person_ids) and int(c) != 0:
            continue
        x1, y1, x2, y2 = map(float, b)
        w, h = max(1.0, x2 - x1), max(1.0, y2 - y1)
        area = w * h
        ar = w / h
        if area < min_area:
            continue
        if not (aspect_min <= ar <= aspect_max):
            continue
        out.append((x1, y1, x2, y2, float(s), int(c)))
    return out


import os
import cv2
import shutil
import subprocess

FFMPEG_PATH = r"C:\ffmpeg\bin\ffmpeg.exe"


def extract_clip(video_path, start_s, end_s, out_path):
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    start_s = max(0.0, float(start_s))
    end_s = max(start_s, float(end_s))
    dur = max(0.10, end_s - start_s)

    # 1) ffmpegë¡œ ë¨¼ì € ì‹œë„ (ì ˆëŒ€ê²½ë¡œ ì‚¬ìš©)
    if os.path.isfile(FFMPEG_PATH):
        cmd = [
            FFMPEG_PATH,
            "-y",
            "-i",
            video_path,  # ë¨¼ì € ì…ë ¥
            "-ss",
            f"{start_s:.3f}",  # ê·¸ ë‹¤ìŒ ì‹œì‘ ì‹œê°„
            "-t",
            f"{dur:.3f}",
            "-c:v",
            "libx264",
            "-crf",
            "23",
            "-pix_fmt",
            "yuv420p",
            "-c:a",
            "aac",
            "-movflags",
            "+faststart",
            "-loglevel",
            "error",
            out_path,
        ]
        try:
            rc = subprocess.run(cmd).returncode
            if rc == 0 and os.path.isfile(out_path):
                return True
        except FileNotFoundError:
            # ffmpeg.exe ê²½ë¡œê°€ ì˜ëª»ëê±°ë‚˜ ì‹¤í–‰ ë¶ˆê°€í•œ ê²½ìš° â†’ ì•„ë˜ OpenCV fallbackìœ¼ë¡œ
            pass

    # 2) OpenCV fallback
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return False

    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    vw = cv2.VideoWriter(out_path, fourcc, fps, (W, H))
    if not vw.isOpened():
        cap.release()
        return False

    start_f = int(round(start_s * fps))
    end_f = int(round(end_s * fps))
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_f)

    fidx = start_f
    ok = True
    while fidx < end_f:
        ok2, fr = cap.read()
        if not ok2:
            ok = False
            break
        vw.write(fr)
        fidx += 1

    vw.release()
    cap.release()
    return ok and os.path.isfile(out_path)


def reencode_to_h264(in_path: str):
    """
    OpenCVë¡œ ë§Œë“  mp4ë¥¼ ffmpegë¡œ H.264(yuv420p)ë¡œ ì¬ì¸ì½”ë”©í•´ì„œ
    ë¸Œë¼ìš°ì €ì—ì„œ ë” ì•ˆì •ì ìœ¼ë¡œ ì¬ìƒë˜ë„ë¡ ë§Œë“ ë‹¤.
    """
    if not (os.path.isfile(FFMPEG_PATH) and os.path.isfile(in_path)):
        return

    tmp_out = in_path + ".h264.mp4"

    cmd = [
        FFMPEG_PATH,
        "-y",
        "-i",
        in_path,
        "-c:v",
        "libx264",
        "-crf",
        "23",
        "-pix_fmt",
        "yuv420p",
        "-c:a",
        "aac",
        "-movflags",
        "+faststart",
        "-loglevel",
        "error",
        tmp_out,
    ]
    rc = subprocess.run(cmd).returncode
    if rc == 0 and os.path.isfile(tmp_out):
        # ì›ë³¸ íŒŒì¼ì„ H.264 ë²„ì „ìœ¼ë¡œ êµì²´
        os.replace(tmp_out, in_path)


def save_thumbnail(video_path, t_sec, out_dir, name_stub):
    os.makedirs(out_dir, exist_ok=True)
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    frame_idx = int(round(max(0.0, t_sec) * fps))
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
    ok, fr = cap.read()
    cap.release()
    if not ok:
        return None
    h, w = fr.shape[:2]
    scale = min(640 / max(1, w), 360 / max(1, h))
    if scale < 1.0:
        fr = cv2.resize(
            fr, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA
        )
    name = f"{name_stub}_thumb.jpg"
    out_path = os.path.join(out_dir, name)
    cv2.imwrite(out_path, fr)
    return out_path


def fmt_time(sec: float):
    sec = max(0.0, float(sec))
    h = int(sec // 3600)
    m = int((sec % 3600) // 60)
    s = int(sec % 60)
    return f"{h:02d}:{m:02d}:{s:02d}"


def fmt_time_cs(sec: float):
    """HH:MM:SS.cc í˜•ì‹ (clips.start_time ìš©)"""
    sec = max(0.0, float(sec))
    h = int(sec // 3600)
    m = int((sec % 3600) // 60)
    s = int(sec % 60)
    cs = int(round((sec - int(sec)) * 100))
    return f"{h:02d}:{m:02d}:{s:02d}.{cs:02d}"


# ================== ì „ì—­ ëª¨ë¸ í•¸ë“¤ ==================
_DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
_SF = None
_CLASSES = None
_T_FAST = None
_ALPHA = None
_SIZE = None
_YOLO = None


def ensure_models_loaded():
    global _SF, _CLASSES, _T_FAST, _ALPHA, _SIZE, _YOLO
    if _YOLO is None:
        _YOLO = YOLO(YOLO_WEIGHTS)
    if _SF is None:
        ckpt = torch.load(CKPT, map_location="cpu", weights_only=False)
        _CLASSES = ckpt.get("classes", ["assault", "swoon", "trespass", "vandalism"])
        _T_FAST = int(ckpt.get("t_fast", T_FAST_FALLBACK))
        _ALPHA = int(ckpt.get("alpha", ALPHA_FALLBACK))
        _SIZE = int(ckpt.get("size", SIZE_FALLBACK))
        model = slowfast_r50(pretrained=False)
        model.blocks[-1].proj = torch.nn.Linear(
            model.blocks[-1].proj.in_features, len(_CLASSES)
        )
        model.load_state_dict(ckpt["state_dict"])
        model.eval().to(_DEVICE)
        _SF = model


# ================== ë¶„ì„ ì½”ì–´ (DB ì—°ë™) ==================
def analyze_core(video_path, job_id: str):
    """
    - stabilized pred_labelë¡œ interval ìƒì„±
    - í´ë¦½/ì¸ë„¤ì¼ ìƒì„±
    - jobs / clips í…Œì´ë¸”ì— ì €ì¥
    """
    ensure_models_loaded()

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        with app.app_context():
            job_row = Job.query.get(job_id)
            if job_row:
                job_row.status = "failed"
                job_row.progress = 0.0
                job_row.message = f"Cannot open video: {video_path}"
                db.session.commit()
        return

    fps = float(cap.get(cv2.CAP_PROP_FPS) or 25.0)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    print("========== DEBUG VIDEO INFO ==========")
    print(f"video_path: {video_path}")
    print(f"fps:          {fps}")
    print(f"total_frames: {total_frames}")
    print(f"W,H:          {W}, {H}")
    print("======================================")

    stem = os.path.splitext(os.path.basename(video_path))[0]
    out_mp4 = os.path.join(ANALYZED_DIR, f"{stem}_analyze.mp4")
    out_csv = os.path.join(CSV_DIR, f"{stem}_stable.csv")
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    writer = cv2.VideoWriter(out_mp4, fourcc, fps, (W, H)) if SAVE_VIDEO else None

    buf = deque(maxlen=_T_FAST)
    rows = []
    no_person_streak = 0
    frame_idx = 0
    p_ema = None
    current_label = ""
    hold_count = 0
    switch_count = 0

    active = None  # {"label": str, "start_f": int, "start_bbox": [x1,y1,x2,y2] | None}
    intervals = []

    try:
        while True:
            ok, frame = cap.read()
            if not ok:
                break
            frame_idx += 1

            # ì§„í–‰ë¥  ê°±ì‹  (10í”„ë ˆì„ë§ˆë‹¤)
            if total_frames > 0 and frame_idx % 10 == 0:
                print(
                    f"[DEBUG] progress trigger: frame_idx={frame_idx}, total_frames={total_frames}"
                )
                with app.app_context():
                    job_row = Job.query.get(job_id)
                    if job_row:
                        job_row.progress = round(100.0 * frame_idx / total_frames, 2)
                        job_row.status = "running"
                        job_row.message = "processing"
                        db.session.commit()

            dets = yolo_person_boxes(
                _YOLO, frame, PERSON_CONF, MIN_AREA_RATIO, ASPECT_MIN, ASPECT_MAX
            )
            roi = make_roi_frame(
                frame,
                dets,
                scale=ROI_SCALE,
                roi_min_ratio=ROI_MIN_AREA_RATIO,
                bg_mode=BG_MODE,
            )

            # === ROI ì—†ì„ ë•Œ: normalë¡œ í‘œì‹œ ===
            if roi is None:
                no_person_streak += 1
                if no_person_streak >= NO_PERSON_CLEAR:
                    buf.clear()
                    p_ema = None
                    current_label = ""
                    hold_count = 0
                    switch_count = 0

                vis = frame.copy()
                cv2.putText(
                    vis,
                    "normal",
                    (12, 28),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.9,
                    (0, 200, 255),
                    2,
                )
                for (x1, y1, x2, y2, *_) in dets:
                    cv2.rectangle(
                        vis, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2
                    )
                if SAVE_VIDEO and writer is not None:
                    writer.write(vis)
                if SAVE_CSV:
                    rows.append({"frame": frame_idx, "pred": "normal", "conf": ""})
                continue
            else:
                no_person_streak = 0

            rgb = center_crop_rgb(roi, _SIZE)
            buf.append(rgb)
            pred_label = ""
            pred_conf = ""

            if len(buf) == _T_FAST:
                fast = (
                    torch.from_numpy(np.stack(list(buf), 0))
                    .permute(3, 0, 1, 2)
                    .unsqueeze(0)
                    .float()
                    / 255.0
                )
                slow = fast[:, :, ::_ALPHA, :, :]
                fast = (fast - MEAN) / STD
                slow = (slow - MEAN) / STD

                with torch.no_grad():
                    logits = _SF([slow.to(_DEVICE), fast.to(_DEVICE)])
                    p = torch.softmax(logits, dim=1)[0].cpu().numpy()

                mask = np.ones_like(p, dtype=np.float32)
                for i, cls in enumerate(_CLASSES):
                    if cls in DISABLE_CLASSES:
                        mask[i] = 0.0
                p = p * mask
                s = float(p.sum())
                if s > 1e-8:
                    p /= s
                else:
                    p[:] = 0.0

                if p_ema is None:
                    p_ema = p.copy()
                else:
                    p_ema = LOGIT_EMA * p_ema + (1.0 - LOGIT_EMA) * p

                order = np.argsort(-p_ema)
                k1 = int(order[0])
                k2 = int(order[1] if len(order) > 1 else order[0])
                top1, top2 = float(p_ema[k1]), float(p_ema[k2])
                label1 = _CLASSES[k1]

                if current_label == "":
                    if top1 >= MIN_SHOW_CONF and (top1 - top2) >= MARGIN_MIN:
                        current_label = label1
                        hold_count = 1
                        switch_count = 0
                else:
                    if label1 == current_label:
                        hold_count += 1
                        switch_count = 0
                    else:
                        curr_idx = _CLASSES.index(current_label)
                        cond_delta = (top1 - float(p_ema[curr_idx])) >= SWITCH_DELTA
                        cond_margin = (top1 - top2) >= MARGIN_MIN
                        cond_hold = hold_count >= MIN_HOLD.get(current_label, 16)
                        if cond_delta and cond_margin and cond_hold:
                            switch_count += 1
                            if switch_count >= SWITCH_CONSEC:
                                current_label = label1
                                hold_count = 1
                                switch_count = 0
                        else:
                            switch_count = 0

                if current_label and top1 >= MIN_SHOW_CONF:
                    pred_label = current_label
                    pred_conf = f"{top1:.2f}"

            # interval on/off
            first_bbox = None
            if dets:
                x1, y1, x2, y2, *_ = dets[0]
                first_bbox = [int(x1), int(y1), int(x2), int(y2)]

            if pred_label:
                if active is None:
                    active = {
                        "label": pred_label,
                        "start_f": max(0, frame_idx - _T_FAST + 1),
                        "start_bbox": first_bbox,
                    }
                else:
                    if active["label"] != pred_label:
                        s = active["start_f"] / fps
                        e = frame_idx / fps
                        intervals.append(
                            {
                                "start": s,
                                "end": e,
                                "label": active["label"],
                                "bbox": active["start_bbox"],
                            }
                        )
                        active = {
                            "label": pred_label,
                            "start_f": max(0, frame_idx - _T_FAST + 1),
                            "start_bbox": first_bbox,
                        }
            else:
                if active is not None:
                    s = active["start_f"] / fps
                    e = frame_idx / fps
                    intervals.append(
                        {
                            "start": s,
                            "end": e,
                            "label": active["label"],
                            "bbox": active["start_bbox"],
                        }
                    )
                    active = None

            # ì˜¤ë²„ë ˆì´/ì €ì¥
            vis = frame.copy()
            # pred_label ì—†ìœ¼ë©´ í™”ë©´ì—ëŠ” normalë¡œ
            title = f"{pred_label} {pred_conf}" if pred_label else "normal"

            for (x1, y1, x2, y2, *_) in dets:
                cv2.rectangle(
                    vis, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2
                )
            cv2.putText(
                vis, title, (12, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 200, 255), 2
            )
            if SAVE_VIDEO and writer is not None:
                writer.write(vis)
            if SAVE_CSV:
                rows.append(
                    {
                        "frame": frame_idx,
                        "pred": pred_label if pred_label else "normal",
                        "conf": pred_conf,
                    }
                )

        # ë£¨í”„ ì¢…ë£Œ í›„ active ë§ˆë¬´ë¦¬
        if active is not None:
            s = active["start_f"] / fps
            e = frame_idx / fps
            intervals.append(
                {
                    "start": s,
                    "end": e,
                    "label": active["label"],
                    "bbox": active["start_bbox"],
                }
            )

    except Exception as e:
        with app.app_context():
            job_row = Job.query.get(job_id)
            if job_row:
                job_row.status = "failed"
                job_row.message = str(e)
                db.session.commit()
        raise
    finally:
        cap.release()
        if writer is not None:
            writer.release()

    if SAVE_VIDEO and os.path.isfile(out_mp4):
        reencode_to_h264(out_mp4)

    # CSV ì €ì¥
    if SAVE_CSV and rows:
        with open(out_csv, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["frame", "pred", "conf"])
            w.writeheader()
            w.writerows(rows)

    # í´ë¦½ / ì¸ë„¤ì¼ ìƒì„±
    clips_meta = []
    for i, it in enumerate(intervals, 1):
        # ğŸ”¹ ì›ë˜ì²˜ëŸ¼ ì›ë³¸ íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ clip ì´ë¦„ ìƒì„±
        base = os.path.splitext(os.path.basename(video_path))[0]
        clip_name = f"{base}_clip{i}.mp4"
        clip_path = os.path.join(EVENT_CLIPS_DIR, clip_name)

        # ğŸ”¥ ë¶„ì„ëœ ì˜ìƒ(out_mp4)ì—ì„œ ìë¥´ê¸°
        ok = extract_clip(out_mp4, it["start"], it["end"], clip_path)

        # ğŸ”¹ ì¸ë„¤ì¼ë„ ë¶„ì„ëœ ì˜ìƒ ê¸°ì¤€
        mid_t = (it["start"] + it["end"]) / 2.0
        thumb_stub = f"{base}_clip{i}"
        thumb_path = save_thumbnail(out_mp4, mid_t, THUMBS_DIR, thumb_stub)

        bbox = it.get("bbox")

        clips_meta.append(
            {
                "ok": ok,
                "label": it["label"],
                "start_sec": it["start"],
                "bbox": bbox,
                "clip_name": clip_name,
                "clip_path": clip_path,
                "thumbnail": thumb_path,
            }
        )
    # DB ë°˜ì˜
    with app.app_context():
        job_row = Job.query.get(job_id)
        if not job_row:
            return

        if SAVE_VIDEO and os.path.isfile(out_mp4):
            job_row.annotated_video = os.path.join(
                "/analyzed_videos", os.path.basename(out_mp4)
            )
        job_row.status = "done"
        job_row.progress = 100.0
        job_row.message = "completed"

        for meta in clips_meta:
            if not meta["ok"]:
                continue
            bbox = meta["bbox"]
            start_x = start_y = start_w = start_h = None
            if bbox is not None:
                x1, y1, x2, y2 = bbox
                start_x, start_y = int(x1), int(y1)
                start_w, start_h = int(x2 - x1), int(y2 - y1)

            clip_row = Clip(
                job_id=job_id,
                class_name=meta["label"],
                checked=False,
                start_time=fmt_time_cs(meta["start_sec"]),
                start_x=start_x,
                start_y=start_y,
                start_w=start_w,
                start_h=start_h,
                clip_name=meta["clip_name"],
                clip_path=meta["clip_path"],
                thumbnail=meta["thumbnail"],
            )
            db.session.add(clip_row)

        db.session.commit()


# ================== Flask Routes ==================
@app.route("/analyze", methods=["POST"])
def analyze():
    """
    ì—…ë¡œë“œ ë˜ëŠ” JSON {"video_path": "...", "username": "..."} ë‘˜ ë‹¤ ì§€ì›
    ì‘ë‹µ: {job_id, status, progress, video_path, username}
    """
    try:
        video_path = None
        username = None

        # form-data ì—…ë¡œë“œ
        if "video" in request.files:
            f = request.files["video"]
            if not f.filename:
                return jsonify({"error": "Empty file name"}), 400
            save_to = os.path.join(UPLOAD_DIR, f.filename)
            f.save(save_to)
            video_path = save_to
            username = request.form.get("username")

        # JSON ìš”ì²­
        if video_path is None:
            data = request.get_json(silent=True) or {}
            video_path = data.get("video_path")
            username = username or data.get("username")

        if not video_path or not os.path.isfile(video_path):
            return (
                jsonify(
                    {
                        "error": "Provide a valid video via form-data 'video' or JSON {'video_path': '...'}"
                    }
                ),
                400,
            )

        if not username:
            username = "guest"

        # DBì— Job ìƒì„± (username ë¬¸ìì—´ë§Œ ì €ì¥)
        with app.app_context():
            job_id = str(uuid.uuid4())
            job_row = Job(
                job_id=job_id,
                username=username,  # models.Jobì— username ì»¬ëŸ¼ ìˆì–´ì•¼ í•¨
                video_path=video_path,
                status="running",
                progress=0.0,
                annotated_video=None,
                message="started",
            )
            db.session.add(job_row)
            db.session.commit()

        # ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹œì‘
        th = threading.Thread(
            target=analyze_core, args=(video_path, job_id), daemon=True
        )
        th.start()

        return jsonify(
            {
                "job_id": job_id,
                "status": "running",
                "progress": 0.0,
                "video_path": video_path,
                "username": username,
            }
        )

    except Exception as e:
        import traceback

        print("\n[ERROR] /analyze ë‚´ë¶€ì—ì„œ ì˜ˆì™¸ ë°œìƒ:")
        traceback.print_exc()  # í„°ë¯¸ë„ì— ì „ì²´ ìŠ¤íƒ ì¶œë ¥
        return (
            jsonify(
                {
                    "error": "internal_error",
                    "detail": str(e),
                }
            ),
            500,
        )


@app.route("/jobs/<job_id>", methods=["GET"])
def get_job(job_id):
    job_row = Job.query.get(job_id)
    if not job_row:
        return jsonify({"error": "job_id not found"}), 404

    # 1) ì›ë˜ ë”•ì…”ë„ˆë¦¬ ê°€ì ¸ì˜¤ê¸°
    data = job_row.to_dict(include_clips=False)

    # 2) DBì— ì €ì¥ëœ ë¶„ì„ì˜ìƒ ê²½ë¡œ ì°¾ê¸°
    #   - í•„ë“œ ì´ë¦„ì´ ì •í™•íˆ ë­”ì§€ ëª¨ë¥´ë‹ˆê¹Œ, ì—¬ëŸ¬ í›„ë³´ë¥¼ ì°¨ë¡€ëŒ€ë¡œ ì‹œë„
    annotated_path = (
        data.get("annotated_path")
        or data.get("annotated_video")
        or data.get("analyzed_video_path")
        or getattr(job_row, "annotated_path", None)
        or getattr(job_row, "annotated_video", None)
        or getattr(job_row, "analyzed_video_path", None)
    )

    # 3) ìƒëŒ€ URLë¡œ ë³€í™˜í•´ì„œ annotated_video_url ì¶”ê°€
    if annotated_path:
        fname = basename(str(annotated_path))  # D:\...\xxx.mp4 -> xxx.mp4
        data["annotated_video_url"] = url_for(
            "serve_analyzed",  # @app.route("/analyzed_videos/<path:fname>")
            fname=fname,
            _external=False,
        )
    else:
        data["annotated_video_url"] = None

    return jsonify(data), 200


import os


@app.route("/jobs/<job_id>/clips", methods=["GET"])
def get_clips_by_job(job_id):
    job = Job.query.filter_by(job_id=job_id).first()
    if not job:
        return jsonify({"detail": "Job not found"}), 404

    clips = Clip.query.filter_by(job_id=job_id).order_by(Clip.start_time).all()
    result = {
        "job_id": job.job_id,
        "video_path": job.video_path,
        "count": len(clips),
        "clips": [],
    }

    from os.path import basename

    for c in clips:
        d = c.to_dict()

        d["checked"] = bool(getattr(c, "checked", 0))

        # ğŸ”¹ 1) xywh â†’ (x1,y1,x2,y2) start_bbox ë¡œ ë³€í™˜
        x = c.start_x  # â† ì—¬ê¸°ë¥¼ ë„¤ ì‹¤ì œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ
        y = c.start_y  # â† ì˜ˆ: c.x, c.y, c.start_x, c.start_y ë“±
        w = c.start_w  # â† ì˜ˆ: c.w, c.width
        h = c.start_h  # â† ì˜ˆ: c.h, c.height

        if None not in (x, y, w, h):
            d["start_bbox"] = {
                "x1": x,
                "y1": y,
                "x2": x + w,
                "y2": y + h,
            }
        else:
            d["start_bbox"] = None

        # ğŸ”¹ 2) í´ë¦½ URL
        clip_name = d.get("clip_name")
        if clip_name:
            d["clip_url"] = url_for("serve_clip", fname=clip_name, _external=False)
        else:
            d["clip_url"] = None

        # ğŸ”¹ 3) ì¸ë„¤ì¼ URL
        thumb_name = d.get("thumbnail") or d.get("thumb_path")
        if thumb_name:
            d["thumb_url"] = url_for(
                "serve_thumb", fname=basename(thumb_name), _external=False
            )
        else:
            d["thumb_url"] = None

        result["clips"].append(d)

    return jsonify(result), 200


# íŒŒì¼ ì„œë¹™
@app.route("/event_clips/<path:fname>", methods=["GET"])
def serve_clip(fname):
    path = os.path.join(EVENT_CLIPS_DIR, fname)
    print("[serve_clip] path =", path, "exists:", os.path.isfile(path))

    if not os.path.isfile(path):
        abort(404)
    return send_from_directory(EVENT_CLIPS_DIR, fname, as_attachment=False)


@app.route("/thumbnails/<path:fname>", methods=["GET"])
def serve_thumb(fname):
    path = os.path.join(THUMBS_DIR, fname)
    if not os.path.isfile(path):
        abort(404)
    return send_from_directory(
        THUMBS_DIR, fname, as_attachment=False, mimetype="image/jpeg"
    )


@app.route("/analyzed_videos/<path:fname>", methods=["GET"])
def serve_analyzed(fname):
    path = os.path.join(ANALYZED_DIR, fname)
    if not os.path.isfile(path):
        abort(404)
    return send_from_directory(ANALYZED_DIR, fname, as_attachment=False)


@app.route("/", methods=["GET"])
def root():
    return jsonify(
        {"name": "AI Security Guard (ASG)", "status": "ok", "base_url": URL_BASE}
    )


@app.route("/clips/<int:clip_id>/check", methods=["PATCH"])
def mark_clip_checked(clip_id):
    clip = Clip.query.get(clip_id)
    if not clip:
        return jsonify({"error": "Clip not found"}), 404
    clip.checked = True

    db.session.commit()
    return jsonify(
        {"message": "checked set to true", "clip_id": clip_id, "checked": True}
    )


@app.route("/jobs/latest", methods=["GET"])
def get_latest_job_for_user():
    """
    username ì¿¼ë¦¬íŒŒë¼ë¯¸í„°ê°€ ì™€ë„ ì¼ë‹¨ì€ ë¬´ì‹œí•˜ê³ ,
    jobs í…Œì´ë¸”ì—ì„œ ê°€ì¥ ìµœê·¼ ê²ƒ í•˜ë‚˜ë§Œ ë°˜í™˜.
    (ë‚˜ì¤‘ì— username ì»¬ëŸ¼ í™•ì‹¤í•´ì§€ë©´ filter_by ì¶”ê°€í•´ë„ OK)
    """
    try:
        q = Job.query

        # ë§Œì•½ Job ëª¨ë¸ì— username ì»¬ëŸ¼ì´ ì‹¤ì œë¡œ ìˆë‹¤ë©´,
        # ì•„ë˜ ì£¼ì„ í’€ê³  usernameìœ¼ë¡œ í•„í„°ë§í•´ë„ ë¨.
        #
        # username = request.args.get("username")
        # if username and hasattr(Job, "username"):
        #     q = q.filter_by(username=username)

        # created_at ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ, ì—†ìœ¼ë©´ id ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        if hasattr(Job, "created_at"):
            q = q.order_by(Job.created_at.desc())
        elif hasattr(Job, "id"):
            q = q.order_by(Job.id.desc())
        else:
            # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ê·¸ëƒ¥ job_id ë¬¸ìì—´ ê¸°ì¤€ìœ¼ë¡œë¼ë„ ì •ë ¬
            q = q.order_by(Job.job_id.desc())

        job_row = q.first()

        if not job_row:
            return jsonify({"error": "no jobs"}), 404

        return jsonify({"job_id": job_row.job_id}), 200

    except Exception as e:
        print("[/jobs/latest] ERROR:", e)
        return jsonify({"error": "internal server error"}), 500


if __name__ == "__main__":
    try:
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
    except Exception:
        pass
    app.run(host="127.0.0.1", port=5001, debug=True)
